{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a564d5f",
   "metadata": {},
   "source": [
    "#  Использование предобученных моделей для классификации изображений\n",
    "\n",
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы: \n",
    "* Deep Learning with PyTorch (2020) Авторы: Eli Stevens, Luca Antiga, Thomas Viehmann \n",
    "* https://pytorch.org/vision/0.16/transforms.html#v2-api-reference-recommended\n",
    "* https://pytorch.org/vision/main/generated/torchvision.datasets.ImageFolder.html\n",
    "* https://pytorch.org/vision/stable/models.html\n",
    "* https://albumentations.ai/docs/getting_started/image_augmentation/\n",
    "* https://www.neurotec.uni-bremen.de/drupal/node/30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecd663",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f6b60d",
   "metadata": {},
   "source": [
    "1\\. Загрузите предобученную модель из `torchvision`. Познакомьтесь с ее архитектурой. Заморозьте веса нескольких слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a22d68",
   "metadata": {},
   "source": [
    "Загрузка предобученных моделей из `torchvision` — это удобный способ использовать уже обученные архитектуры для вашей задачи. \n",
    "\n",
    "Что нужно знать:\n",
    "\n",
    "- torchvision предоставляет множество предобученных моделей для различных задач: классификация, обнаружение объектов, сегментация.\n",
    "- при загрузке модели вы можете указать, какие именно веса использовать; для некоторых моделей есть варианты\n",
    "- предобученные модели могут служить отличной отправной точкой для решения вашей задачи\n",
    "- предобученную модель нужно выбирать, исходя \n",
    "    - вашей задачи\n",
    "    - того, на чем она училась и\n",
    "    - ваших ресурсов (мощность машины)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a99255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch as th\n",
    "\n",
    "model = models.efficientnet_b1(weights=models.EfficientNet_B1_Weights.DEFAULT )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833d0341",
   "metadata": {},
   "source": [
    "Вы можете дообучить предобученную модель на своем наборе данных, что часто приводит к лучшим результатам, чем обучение с нуля. Это особенно полезно, если ваш набор данных мал или схож с тем, на котором модель была обучена.\n",
    "\n",
    "Часто необходимо заменить последний полносвязный слой модели, чтобы он соответствовал числу классов в вашей задаче. Это легко сделать, просто нужно понять, как к нему обратиться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdbce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "            (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(32, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(8, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(16, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (2): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.008695652173913044, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "            (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(96, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(4, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.017391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.026086956521739136, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.034782608695652174, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=144, bias=False)\n",
       "            (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(144, 6, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(6, 144, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05217391304347827, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06086956521739131, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(240, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(10, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06956521739130435, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0782608695652174, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09565217391304348, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
       "            (1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(480, 20, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(20, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10434782608695654, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11304347826086956, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12173913043478261, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(672, 28, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(28, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1391304347826087, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14782608695652175, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1565217391304348, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16521739130434784, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 1152, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1152, bias=False)\n",
       "            (1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1152, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(48, 1152, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1826086956521739, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(320, 1920, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1920, 1920, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1920, bias=False)\n",
       "            (1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1920, 80, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(80, 1920, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1920, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19130434782608696, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (8): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028ab0e3",
   "metadata": {},
   "source": [
    "Нас интересует последний блок `classifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae9a0713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "num_classes = 10\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bc142",
   "metadata": {},
   "source": [
    "Теперь модель будет выдавать 10 чисел вместо 1000. Но новый вариант `nn.Linear` пока инициализирован случайным образом. Чтобы прогнозы были адекватные, модель надо дообучить. Варианты:\n",
    "\n",
    "1. Просто берем эту модель и используем в стандартном цикле обучения. Обучаться будет последний слой, но вместе с ним и все предыдущие.\n",
    "2. \"Заморозить\" все слои, кроме последнего (нового). Тогда обучаться будет последний слой, а остальные - не будут меняться\n",
    "3. Промежуточный вариант: заморозить часть верхних слоев.\n",
    "\n",
    "Что выбрать? Обычно:\n",
    "-  более глубокие слои (ближе к выходу) отвечают за более специфические признаки, связанные с конкретной задачей (например, распознавание объектов) - эти слои часто требуют дообучения.\n",
    "- первые слои обычно захватывают более общие признаки (например, края и текстуры), которые могут быть полезны для множества задач - их часто замораживают, чтобы сохранить эти общие представления.\n",
    "\n",
    "Базовые интуиции:\n",
    "- чем больше данных, тем больше можно оставить размороженным;\n",
    "- в зависимости от того, насколько ваша задача близка к тому, на чем училась модель изначально, вы можете заморозить больше или меньше слоев\n",
    "\n",
    "В итоге все сводится к тому, что вы должны экспериментировать и измерять качество.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a198ebb",
   "metadata": {},
   "source": [
    "Как замораживать: нужно соответствующему весу указать `requires_grad = False`. Обычно удобно сделать условие по названию. Названия параметров вместе с самими тензорами можно получить при помощи `named_parameters`.\n",
    "\n",
    "Рекомендую использовать именно метод `requires_grad_` (пример ниже), так как при простом присваивании может закрасться ошибка (если нет средств анализа кода)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e88dc9e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.0.0.weight',\n",
       " 'features.0.1.weight',\n",
       " 'features.0.1.bias',\n",
       " 'features.1.0.block.0.0.weight',\n",
       " 'features.1.0.block.0.1.weight',\n",
       " 'features.1.0.block.0.1.bias',\n",
       " 'features.1.0.block.1.fc1.weight',\n",
       " 'features.1.0.block.1.fc1.bias',\n",
       " 'features.1.0.block.1.fc2.weight',\n",
       " 'features.1.0.block.1.fc2.bias',\n",
       " 'features.1.0.block.2.0.weight',\n",
       " 'features.1.0.block.2.1.weight',\n",
       " 'features.1.0.block.2.1.bias',\n",
       " 'features.1.1.block.0.0.weight',\n",
       " 'features.1.1.block.0.1.weight',\n",
       " 'features.1.1.block.0.1.bias',\n",
       " 'features.1.1.block.1.fc1.weight',\n",
       " 'features.1.1.block.1.fc1.bias',\n",
       " 'features.1.1.block.1.fc2.weight',\n",
       " 'features.1.1.block.1.fc2.bias',\n",
       " 'features.1.1.block.2.0.weight',\n",
       " 'features.1.1.block.2.1.weight',\n",
       " 'features.1.1.block.2.1.bias',\n",
       " 'features.2.0.block.0.0.weight',\n",
       " 'features.2.0.block.0.1.weight',\n",
       " 'features.2.0.block.0.1.bias',\n",
       " 'features.2.0.block.1.0.weight',\n",
       " 'features.2.0.block.1.1.weight',\n",
       " 'features.2.0.block.1.1.bias',\n",
       " 'features.2.0.block.2.fc1.weight',\n",
       " 'features.2.0.block.2.fc1.bias',\n",
       " 'features.2.0.block.2.fc2.weight',\n",
       " 'features.2.0.block.2.fc2.bias',\n",
       " 'features.2.0.block.3.0.weight',\n",
       " 'features.2.0.block.3.1.weight',\n",
       " 'features.2.0.block.3.1.bias',\n",
       " 'features.2.1.block.0.0.weight',\n",
       " 'features.2.1.block.0.1.weight',\n",
       " 'features.2.1.block.0.1.bias',\n",
       " 'features.2.1.block.1.0.weight',\n",
       " 'features.2.1.block.1.1.weight',\n",
       " 'features.2.1.block.1.1.bias',\n",
       " 'features.2.1.block.2.fc1.weight',\n",
       " 'features.2.1.block.2.fc1.bias',\n",
       " 'features.2.1.block.2.fc2.weight',\n",
       " 'features.2.1.block.2.fc2.bias',\n",
       " 'features.2.1.block.3.0.weight',\n",
       " 'features.2.1.block.3.1.weight',\n",
       " 'features.2.1.block.3.1.bias',\n",
       " 'features.2.2.block.0.0.weight',\n",
       " 'features.2.2.block.0.1.weight',\n",
       " 'features.2.2.block.0.1.bias',\n",
       " 'features.2.2.block.1.0.weight',\n",
       " 'features.2.2.block.1.1.weight',\n",
       " 'features.2.2.block.1.1.bias',\n",
       " 'features.2.2.block.2.fc1.weight',\n",
       " 'features.2.2.block.2.fc1.bias',\n",
       " 'features.2.2.block.2.fc2.weight',\n",
       " 'features.2.2.block.2.fc2.bias',\n",
       " 'features.2.2.block.3.0.weight',\n",
       " 'features.2.2.block.3.1.weight',\n",
       " 'features.2.2.block.3.1.bias',\n",
       " 'features.3.0.block.0.0.weight',\n",
       " 'features.3.0.block.0.1.weight',\n",
       " 'features.3.0.block.0.1.bias',\n",
       " 'features.3.0.block.1.0.weight',\n",
       " 'features.3.0.block.1.1.weight',\n",
       " 'features.3.0.block.1.1.bias',\n",
       " 'features.3.0.block.2.fc1.weight',\n",
       " 'features.3.0.block.2.fc1.bias',\n",
       " 'features.3.0.block.2.fc2.weight',\n",
       " 'features.3.0.block.2.fc2.bias',\n",
       " 'features.3.0.block.3.0.weight',\n",
       " 'features.3.0.block.3.1.weight',\n",
       " 'features.3.0.block.3.1.bias',\n",
       " 'features.3.1.block.0.0.weight',\n",
       " 'features.3.1.block.0.1.weight',\n",
       " 'features.3.1.block.0.1.bias',\n",
       " 'features.3.1.block.1.0.weight',\n",
       " 'features.3.1.block.1.1.weight',\n",
       " 'features.3.1.block.1.1.bias',\n",
       " 'features.3.1.block.2.fc1.weight',\n",
       " 'features.3.1.block.2.fc1.bias',\n",
       " 'features.3.1.block.2.fc2.weight',\n",
       " 'features.3.1.block.2.fc2.bias',\n",
       " 'features.3.1.block.3.0.weight',\n",
       " 'features.3.1.block.3.1.weight',\n",
       " 'features.3.1.block.3.1.bias',\n",
       " 'features.3.2.block.0.0.weight',\n",
       " 'features.3.2.block.0.1.weight',\n",
       " 'features.3.2.block.0.1.bias',\n",
       " 'features.3.2.block.1.0.weight',\n",
       " 'features.3.2.block.1.1.weight',\n",
       " 'features.3.2.block.1.1.bias',\n",
       " 'features.3.2.block.2.fc1.weight',\n",
       " 'features.3.2.block.2.fc1.bias',\n",
       " 'features.3.2.block.2.fc2.weight',\n",
       " 'features.3.2.block.2.fc2.bias',\n",
       " 'features.3.2.block.3.0.weight',\n",
       " 'features.3.2.block.3.1.weight',\n",
       " 'features.3.2.block.3.1.bias',\n",
       " 'features.4.0.block.0.0.weight',\n",
       " 'features.4.0.block.0.1.weight',\n",
       " 'features.4.0.block.0.1.bias',\n",
       " 'features.4.0.block.1.0.weight',\n",
       " 'features.4.0.block.1.1.weight',\n",
       " 'features.4.0.block.1.1.bias',\n",
       " 'features.4.0.block.2.fc1.weight',\n",
       " 'features.4.0.block.2.fc1.bias',\n",
       " 'features.4.0.block.2.fc2.weight',\n",
       " 'features.4.0.block.2.fc2.bias',\n",
       " 'features.4.0.block.3.0.weight',\n",
       " 'features.4.0.block.3.1.weight',\n",
       " 'features.4.0.block.3.1.bias',\n",
       " 'features.4.1.block.0.0.weight',\n",
       " 'features.4.1.block.0.1.weight',\n",
       " 'features.4.1.block.0.1.bias',\n",
       " 'features.4.1.block.1.0.weight',\n",
       " 'features.4.1.block.1.1.weight',\n",
       " 'features.4.1.block.1.1.bias',\n",
       " 'features.4.1.block.2.fc1.weight',\n",
       " 'features.4.1.block.2.fc1.bias',\n",
       " 'features.4.1.block.2.fc2.weight',\n",
       " 'features.4.1.block.2.fc2.bias',\n",
       " 'features.4.1.block.3.0.weight',\n",
       " 'features.4.1.block.3.1.weight',\n",
       " 'features.4.1.block.3.1.bias',\n",
       " 'features.4.2.block.0.0.weight',\n",
       " 'features.4.2.block.0.1.weight',\n",
       " 'features.4.2.block.0.1.bias',\n",
       " 'features.4.2.block.1.0.weight',\n",
       " 'features.4.2.block.1.1.weight',\n",
       " 'features.4.2.block.1.1.bias',\n",
       " 'features.4.2.block.2.fc1.weight',\n",
       " 'features.4.2.block.2.fc1.bias',\n",
       " 'features.4.2.block.2.fc2.weight',\n",
       " 'features.4.2.block.2.fc2.bias',\n",
       " 'features.4.2.block.3.0.weight',\n",
       " 'features.4.2.block.3.1.weight',\n",
       " 'features.4.2.block.3.1.bias',\n",
       " 'features.4.3.block.0.0.weight',\n",
       " 'features.4.3.block.0.1.weight',\n",
       " 'features.4.3.block.0.1.bias',\n",
       " 'features.4.3.block.1.0.weight',\n",
       " 'features.4.3.block.1.1.weight',\n",
       " 'features.4.3.block.1.1.bias',\n",
       " 'features.4.3.block.2.fc1.weight',\n",
       " 'features.4.3.block.2.fc1.bias',\n",
       " 'features.4.3.block.2.fc2.weight',\n",
       " 'features.4.3.block.2.fc2.bias',\n",
       " 'features.4.3.block.3.0.weight',\n",
       " 'features.4.3.block.3.1.weight',\n",
       " 'features.4.3.block.3.1.bias',\n",
       " 'features.5.0.block.0.0.weight',\n",
       " 'features.5.0.block.0.1.weight',\n",
       " 'features.5.0.block.0.1.bias',\n",
       " 'features.5.0.block.1.0.weight',\n",
       " 'features.5.0.block.1.1.weight',\n",
       " 'features.5.0.block.1.1.bias',\n",
       " 'features.5.0.block.2.fc1.weight',\n",
       " 'features.5.0.block.2.fc1.bias',\n",
       " 'features.5.0.block.2.fc2.weight',\n",
       " 'features.5.0.block.2.fc2.bias',\n",
       " 'features.5.0.block.3.0.weight',\n",
       " 'features.5.0.block.3.1.weight',\n",
       " 'features.5.0.block.3.1.bias',\n",
       " 'features.5.1.block.0.0.weight',\n",
       " 'features.5.1.block.0.1.weight',\n",
       " 'features.5.1.block.0.1.bias',\n",
       " 'features.5.1.block.1.0.weight',\n",
       " 'features.5.1.block.1.1.weight',\n",
       " 'features.5.1.block.1.1.bias',\n",
       " 'features.5.1.block.2.fc1.weight',\n",
       " 'features.5.1.block.2.fc1.bias',\n",
       " 'features.5.1.block.2.fc2.weight',\n",
       " 'features.5.1.block.2.fc2.bias',\n",
       " 'features.5.1.block.3.0.weight',\n",
       " 'features.5.1.block.3.1.weight',\n",
       " 'features.5.1.block.3.1.bias',\n",
       " 'features.5.2.block.0.0.weight',\n",
       " 'features.5.2.block.0.1.weight',\n",
       " 'features.5.2.block.0.1.bias',\n",
       " 'features.5.2.block.1.0.weight',\n",
       " 'features.5.2.block.1.1.weight',\n",
       " 'features.5.2.block.1.1.bias',\n",
       " 'features.5.2.block.2.fc1.weight',\n",
       " 'features.5.2.block.2.fc1.bias',\n",
       " 'features.5.2.block.2.fc2.weight',\n",
       " 'features.5.2.block.2.fc2.bias',\n",
       " 'features.5.2.block.3.0.weight',\n",
       " 'features.5.2.block.3.1.weight',\n",
       " 'features.5.2.block.3.1.bias',\n",
       " 'features.5.3.block.0.0.weight',\n",
       " 'features.5.3.block.0.1.weight',\n",
       " 'features.5.3.block.0.1.bias',\n",
       " 'features.5.3.block.1.0.weight',\n",
       " 'features.5.3.block.1.1.weight',\n",
       " 'features.5.3.block.1.1.bias',\n",
       " 'features.5.3.block.2.fc1.weight',\n",
       " 'features.5.3.block.2.fc1.bias',\n",
       " 'features.5.3.block.2.fc2.weight',\n",
       " 'features.5.3.block.2.fc2.bias',\n",
       " 'features.5.3.block.3.0.weight',\n",
       " 'features.5.3.block.3.1.weight',\n",
       " 'features.5.3.block.3.1.bias',\n",
       " 'features.6.0.block.0.0.weight',\n",
       " 'features.6.0.block.0.1.weight',\n",
       " 'features.6.0.block.0.1.bias',\n",
       " 'features.6.0.block.1.0.weight',\n",
       " 'features.6.0.block.1.1.weight',\n",
       " 'features.6.0.block.1.1.bias',\n",
       " 'features.6.0.block.2.fc1.weight',\n",
       " 'features.6.0.block.2.fc1.bias',\n",
       " 'features.6.0.block.2.fc2.weight',\n",
       " 'features.6.0.block.2.fc2.bias',\n",
       " 'features.6.0.block.3.0.weight',\n",
       " 'features.6.0.block.3.1.weight',\n",
       " 'features.6.0.block.3.1.bias',\n",
       " 'features.6.1.block.0.0.weight',\n",
       " 'features.6.1.block.0.1.weight',\n",
       " 'features.6.1.block.0.1.bias',\n",
       " 'features.6.1.block.1.0.weight',\n",
       " 'features.6.1.block.1.1.weight',\n",
       " 'features.6.1.block.1.1.bias',\n",
       " 'features.6.1.block.2.fc1.weight',\n",
       " 'features.6.1.block.2.fc1.bias',\n",
       " 'features.6.1.block.2.fc2.weight',\n",
       " 'features.6.1.block.2.fc2.bias',\n",
       " 'features.6.1.block.3.0.weight',\n",
       " 'features.6.1.block.3.1.weight',\n",
       " 'features.6.1.block.3.1.bias',\n",
       " 'features.6.2.block.0.0.weight',\n",
       " 'features.6.2.block.0.1.weight',\n",
       " 'features.6.2.block.0.1.bias',\n",
       " 'features.6.2.block.1.0.weight',\n",
       " 'features.6.2.block.1.1.weight',\n",
       " 'features.6.2.block.1.1.bias',\n",
       " 'features.6.2.block.2.fc1.weight',\n",
       " 'features.6.2.block.2.fc1.bias',\n",
       " 'features.6.2.block.2.fc2.weight',\n",
       " 'features.6.2.block.2.fc2.bias',\n",
       " 'features.6.2.block.3.0.weight',\n",
       " 'features.6.2.block.3.1.weight',\n",
       " 'features.6.2.block.3.1.bias',\n",
       " 'features.6.3.block.0.0.weight',\n",
       " 'features.6.3.block.0.1.weight',\n",
       " 'features.6.3.block.0.1.bias',\n",
       " 'features.6.3.block.1.0.weight',\n",
       " 'features.6.3.block.1.1.weight',\n",
       " 'features.6.3.block.1.1.bias',\n",
       " 'features.6.3.block.2.fc1.weight',\n",
       " 'features.6.3.block.2.fc1.bias',\n",
       " 'features.6.3.block.2.fc2.weight',\n",
       " 'features.6.3.block.2.fc2.bias',\n",
       " 'features.6.3.block.3.0.weight',\n",
       " 'features.6.3.block.3.1.weight',\n",
       " 'features.6.3.block.3.1.bias',\n",
       " 'features.6.4.block.0.0.weight',\n",
       " 'features.6.4.block.0.1.weight',\n",
       " 'features.6.4.block.0.1.bias',\n",
       " 'features.6.4.block.1.0.weight',\n",
       " 'features.6.4.block.1.1.weight',\n",
       " 'features.6.4.block.1.1.bias',\n",
       " 'features.6.4.block.2.fc1.weight',\n",
       " 'features.6.4.block.2.fc1.bias',\n",
       " 'features.6.4.block.2.fc2.weight',\n",
       " 'features.6.4.block.2.fc2.bias',\n",
       " 'features.6.4.block.3.0.weight',\n",
       " 'features.6.4.block.3.1.weight',\n",
       " 'features.6.4.block.3.1.bias',\n",
       " 'features.7.0.block.0.0.weight',\n",
       " 'features.7.0.block.0.1.weight',\n",
       " 'features.7.0.block.0.1.bias',\n",
       " 'features.7.0.block.1.0.weight',\n",
       " 'features.7.0.block.1.1.weight',\n",
       " 'features.7.0.block.1.1.bias',\n",
       " 'features.7.0.block.2.fc1.weight',\n",
       " 'features.7.0.block.2.fc1.bias',\n",
       " 'features.7.0.block.2.fc2.weight',\n",
       " 'features.7.0.block.2.fc2.bias',\n",
       " 'features.7.0.block.3.0.weight',\n",
       " 'features.7.0.block.3.1.weight',\n",
       " 'features.7.0.block.3.1.bias',\n",
       " 'features.7.1.block.0.0.weight',\n",
       " 'features.7.1.block.0.1.weight',\n",
       " 'features.7.1.block.0.1.bias',\n",
       " 'features.7.1.block.1.0.weight',\n",
       " 'features.7.1.block.1.1.weight',\n",
       " 'features.7.1.block.1.1.bias',\n",
       " 'features.7.1.block.2.fc1.weight',\n",
       " 'features.7.1.block.2.fc1.bias',\n",
       " 'features.7.1.block.2.fc2.weight',\n",
       " 'features.7.1.block.2.fc2.bias',\n",
       " 'features.7.1.block.3.0.weight',\n",
       " 'features.7.1.block.3.1.weight',\n",
       " 'features.7.1.block.3.1.bias',\n",
       " 'features.8.0.weight',\n",
       " 'features.8.1.weight',\n",
       " 'features.8.1.bias',\n",
       " 'classifier.1.weight',\n",
       " 'classifier.1.bias']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(n for n, _ in model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "23650231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbd6b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример\n",
    "for n, w in model.named_parameters():\n",
    "    if not n.startswith(\"classifier\"):\n",
    "        w.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0cae001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60691dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# пример с ошибкой (рекомендую не присваивать напрямую)\n",
    "for n, w in model.named_parameters():\n",
    "    if not n.startswith(\"classifier\"):\n",
    "        w.required_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33311186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features[0][0].weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbceb43",
   "metadata": {},
   "source": [
    "При использовании предобученных моделей важно применять те же преобразования, которые были использованы при их обучении. Обычно это изменение размера (224x224 для многих моделей) и нормализация, но может быть что-то более сложное. Рекомендую отталкиваться от реализации той модели, которую вы загрузили. В `torchvision` можно достать объект для преобразования, а если берете откуда-то из другого места, то нужно читать документацию или (что бывает чаще) читать код.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e146dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[240]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = models.EfficientNet_B1_Weights.IMAGENET1K_V1.transforms()\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b6d63",
   "metadata": {},
   "source": [
    "## Задачи для самостоятельного решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca02454",
   "metadata": {},
   "source": [
    "<p class=\"task\" id=\"1\"></p>\n",
    "\n",
    "1\\. Используя реализацию из `torchvision`, cоздайте модель `vgg16` и загрузите предобученные веса `IMAGENET1K_V1`. Выведите на экран структуру модели, количество слоев и количество настраиваемых (`requires_grad==True`) параметров модели. \n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b918e12",
   "metadata": {},
   "source": [
    "<p class=\"task\" id=\"2\"></p>\n",
    "\n",
    "2\\. Создайте датасет `CatBreeds` на основе данных из архива `cat_breeds_4.zip`. Разбейте датасет на обучающее и тестовое множество в соотношении 80 на 20%. \n",
    "\n",
    "К обучающему датасету примените следующее преобразование: приведите картинки к размеру 256x256, затем обрежьте по центру с размером 224х224, затем переведите изображения в тензор и нормализуйте значения интенсивности пикселей (`mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)`).\n",
    "\n",
    "К тестовому датасету примените преобразование `VGG16_Weights.IMAGENET1K_V1.transforms`.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef2afa",
   "metadata": {},
   "source": [
    "<p class=\"task\" id=\"3\"></p>\n",
    "\n",
    "3\\. Заморозьте все веса модели из предыдущего задания. Замените последний слой `Linear` классификатора на новый слой, соответствующий задаче. После изменения последнего слоя выведите на экран количество настраиваемых (`requires_grad==True`) параметров модели. Решите задачу, используя модель с замороженными весами и изменнным последним слоем. \n",
    "\n",
    "Постройте график изменения значения функции потерь на обучающем множестве в зависимости от номера эпохи, графики изменения метрики accuracy на обучающем в зависимости от эпохи. Выведите на экран итоговое значение метрики accuracy на обучающем и тестовом множестве. \n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecc37d5",
   "metadata": {},
   "source": [
    "<p class=\"task\" id=\"4\"></p>\n",
    "\n",
    "4\\. Повторите решение предыдущей задачи, заморозив все сверточные слои, кроме последнего (слои классификатора не замораживайте). Сравните качество полученного решения и решения из предыдущей задачи, а также время, затраченное на обучения моделей. Перед началом работы создайте модель заново.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad15b2",
   "metadata": {},
   "source": [
    "<p class=\"task\" id=\"5\"></p>\n",
    "\n",
    "5\\. Повторите решение задачи 3, расширив обучающий набор данных при помощи преобразований из `torchvision`, изменяющих изображение (повороты, изменение интенсивности пикселей, обрезание и т.д.). При оценке модели на тестовой выборке данные преобразования применяться не должны. Решение о том, сколько и каких слоев модели будет обучаться, примите самостоятельно. Перед началом работы создайте модель заново.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

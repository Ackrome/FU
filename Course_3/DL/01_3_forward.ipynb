{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a564d5f",
   "metadata": {},
   "source": [
    "#  Forward pass\n",
    "\n",
    "__Автор задач: Блохин Н.В. (NVBlokhin@fa.ru)__\n",
    "\n",
    "Материалы: \n",
    "* Deep Learning with PyTorch (2020) Авторы: Eli Stevens, Luca Antiga, Thomas Viehmann \n",
    "* https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
    "* https://machinelearningmastery.com/choose-an-activation-function-for-deep-learning/\n",
    "* https://machinelearningmastery.com/loss-and-loss-functions-for-training-deep-learning-neural-networks/\n",
    "* https://kidger.site/thoughts/jaxtyping/\n",
    "* https://github.com/patrick-kidger/torchtyping/tree/master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ecd663",
   "metadata": {},
   "source": [
    "## Задачи для совместного разбора"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87d01a9",
   "metadata": {
    "id": "_2ArJn_nsdZC"
   },
   "source": [
    "1\\. Используя операции над матрицами и векторами из библиотеки `torch`, реализуйте нейрон с заданными весами `weights` и `bias`. Пропустите вектор `inputs` через нейрон и выведите результат. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1b50d5",
   "metadata": {
    "id": "f4agkY9WqPwe"
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "class Neuron:\n",
    "    def __init__(self, n_features: int, bias: bool = True):\n",
    "        # <создать атрибуты объекта weights и bias>\n",
    "        self.weights: th.Tensor = th.randn(n_features)\n",
    "        if bias:\n",
    "            self.bias: float = th.randn(1)\n",
    "        else:\n",
    "            self.bias = 0\n",
    "\n",
    "    def forward(self, inputs: th.Tensor) -> th.Tensor:\n",
    "        # inputs: (n_features, )\n",
    "        y = self.weights @ inputs + self.bias\n",
    "        \n",
    "        # returns scalar\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60f299f7",
   "metadata": {
    "id": "HJRkSkHHsb7u"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.9957])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "inputs = th.tensor([1.0, 2.0, 3.0, 4.0])\n",
    "\n",
    "neuron = Neuron(\n",
    "    n_features=len(inputs),\n",
    "    bias = True\n",
    ")\n",
    "\n",
    "neuron.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5fe51",
   "metadata": {
    "id": "B9kngE6Fxs9D"
   },
   "source": [
    "2\\. Используя операции над матрицами и векторами из библиотеки `torch`, реализуйте функцию активации ReLU:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f4353f4e3e484130504049599d2e7b040793e1eb)\n",
    "\n",
    "Создайте матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверьте работоспособность функции активации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4679f4e5",
   "metadata": {
    "id": "jZLvMRByxSTC"
   },
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, inputs: th.Tensor) -> th.Tensor:\n",
    "        # <реализовать логику ReLU>\n",
    "        # inputs: (n_features, )\n",
    "        # returns: (n_features, )\n",
    "        return inputs.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b93844",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7687, 0.4377, 1.0318],\n",
       "        [0.0000, 0.6162, 0.0000],\n",
       "        [0.0000, 0.0342, 0.0341],\n",
       "        [0.0000, 1.1235, 0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = th.randn(4,3)\n",
    "act = ReLU()\n",
    "act.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a16748",
   "metadata": {
    "id": "EY-k3eEs0f7f"
   },
   "source": [
    "3\\. Используя операции над матрицами и векторами из библиотеки `torch`, реализуйте функцию потерь MSE:\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/e258221518869aa1c6561bb75b99476c4734108e)\n",
    "где $Y_i$ - правильный ответ для примера $i$, $\\hat{Y_i}$ - предсказание модели для примера $i$, $n$ - количество примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e046dfa6",
   "metadata": {
    "id": "f9-wdj5Tz-br"
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y_pred: th.Tensor, y_true: th.Tensor) -> th.Tensor:\n",
    "        # y_pred: (batch_size, )\n",
    "        # y_true: (batch_size, )\n",
    "        # returns: scalar\n",
    "        return (y_true - y_pred).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e686f8b8",
   "metadata": {
    "id": "NAyuDU9F1Vuz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6667)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = th.tensor([1.0, 3.0, 5.0])\n",
    "y_true = th.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "MSELoss().forward(y_pred,y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7b6d63",
   "metadata": {},
   "source": [
    "## Задачи для самостоятельного решения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e742b",
   "metadata": {
    "id": "0J2RM8f5wP33"
   },
   "source": [
    "### Cоздание полносвязных слоев"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fe867c",
   "metadata": {
    "id": "OQ2OxH4_vBLu"
   },
   "source": [
    "<p class=\"task\" id=\"1\"></p>\n",
    "\n",
    "1\\. Используя операции над тензорами из библиотеки `torch`, реализуйте полносвязный слой из `n_neurons` нейронов с `n_features` весами у каждого нейрона (инициализируются из стандартного нормального распределения) и опциональным вектором смещения. Для определения слоя не используйте готовые функции или слои из `torch`, реализуйте слой, используя только операции над тензорами.\n",
    "\n",
    "$$y = xW^T + b$$\n",
    "\n",
    "Пропустите вектор `inputs` через слой и выведите результат. Результатом прогона сквозь слой должна быть матрица размера `batch_size` x `n_neurons`.\n",
    "\n",
    "- [x] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "680571a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size x n_features = torch.Size([5, 6])\n",
      "batch_size x n_neurons = torch.Size([5, 7])\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "class Linear:\n",
    "    def __init__(self, n_neurons: int, n_features: int, bias: bool = False) -> None:\n",
    "        self.n_features = n_features\n",
    "        self.n_neurons = n_neurons\n",
    "\n",
    "        self.weights: th.Tensor = th.randn(n_neurons,n_features)\n",
    "        if bias:\n",
    "            self.bias: float = th.randn(1)\n",
    "        else:\n",
    "            self.bias = 0\n",
    "\n",
    "    def forward(self, inputs: th.Tensor) -> th.Tensor:\n",
    "        # inputs: (batch_size, n_features)\n",
    "        # returns: (batch_size, n_neurons)\n",
    "        return inputs @  self.weights.T + self.bias\n",
    "\n",
    "inputs = th.randn(5,6)\n",
    "n_neurons = 7\n",
    "print(f'batch_size x n_features = {inputs.shape}')\n",
    "\n",
    "nthlin = Linear(n_neurons,inputs.shape[1],True)\n",
    "\n",
    "res = nthlin.forward(inputs)\n",
    "print(f'batch_size x n_neurons = {res.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad52a4f",
   "metadata": {
    "id": "IPG4UqL4wajI"
   },
   "source": [
    "<p class=\"task\" id=\"2\"></p>\n",
    "\n",
    "2\\. Используя решение предыдущей задачи, создайте 2 полносвязных слоя и пропустите тензор `inputs` последовательно через эти два слоя. Количество нейронов в первом слое выберите произвольно, количество нейронов во втором слое выберите так, чтобы результатом прогона являлась матрица `batch_size x 7`. \n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e79b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size x n_features = torch.Size([50, 60])\n",
      "batch_size x 7 = torch.Size([50, 7])\n"
     ]
    }
   ],
   "source": [
    "inputs = th.randn(50,60)\n",
    "n_neurons1 = 20\n",
    "\n",
    "print(f'batch_size x n_features = {inputs.shape}')\n",
    "\n",
    "\n",
    "nthlin1 = Linear(n_neurons1,inputs.shape[1],True)\n",
    "res1 = nthlin1.forward(inputs)\n",
    "\n",
    "nthlin2 = Linear(7,res1.shape[1],True)\n",
    "res2 = nthlin2.forward(res1)\n",
    "print(f'batch_size x 7 = {res2.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89bb8e",
   "metadata": {
    "id": "cRVH_2K7xTBC"
   },
   "source": [
    "### Создание функций активации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c912a6",
   "metadata": {
    "id": "puExCWiKyTtb"
   },
   "source": [
    "<p class=\"task\" id=\"3\"></p>\n",
    "\n",
    "3\\. Используя операции над тензорами из библиотеки `torch`, реализуйте функцию активации softmax:\n",
    "\n",
    "$$f_i(\\vec{x}) = \\frac{e^{x_i}}{\\sum_{j=1}^J e^{x_j}}$$\n",
    "\n",
    "$$\\overrightarrow{x} = (x_1, ..., x_J)$$\n",
    "\n",
    "Создайте матрицу размера (4,3), заполненную числами из стандартного нормального распределения, и проверьте работоспособность функции активации. Строки матрицы трактовать как выходы линейного слоя некоторого классификатора для 4 различных примеров. Функция должна применяться переданной на вход матрице построчно. Для определения слоя не используйте готовые функции или слои из `torch`, реализующие данную функцию активации.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41ec4062",
   "metadata": {
    "id": "fXNcFlqqyKHl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6341, 0.1356, 0.2303],\n",
       "        [0.2099, 0.4611, 0.3290],\n",
       "        [0.6463, 0.1357, 0.2180],\n",
       "        [0.2647, 0.2855, 0.4498]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "class Softmax:\n",
    "    def forward(self, inputs: th.Tensor) -> th.Tensor:\n",
    "        # <реализовать логику Softmax>\n",
    "        # inputs: (batch_size, n_features)\n",
    "        # returns: (batch_size, n_features)\n",
    "        exp = th.exp(inputs)\n",
    "        return exp / th.sum(exp,1, keepdim=True)\n",
    "    \n",
    "inputs = th.randn(4,3)\n",
    "    \n",
    "sm = Softmax()\n",
    "sm.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8acbae",
   "metadata": {
    "id": "vxVK2TYez_Ye"
   },
   "source": [
    "<p class=\"task\" id=\"4\"></p>\n",
    "\n",
    "4 Используя операции над тензорами из библиотеки `torch`, реализуйте функцию активации ELU:\n",
    "\n",
    "$$f(\\alpha,x) = \\begin{cases}\\alpha(e^x - 1) \\\\ x\\end{cases} ~ \\begin{gather}x <0 \\\\ x \\ge 0\\end{gather}$$\n",
    "\n",
    "Создайте матрицу размера 4x3, заполненную числами из стандартного нормального распределения, и проверьте работоспособность функции активации. Для определения слоя не используйте готовые функции или слои из `torch`, реализующие данную функцию активации.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "998b3675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0730,  0.2091,  0.5829],\n",
       "        [-0.0737, -0.0566,  0.2634],\n",
       "        [ 1.0739,  0.0409,  2.1821],\n",
       "        [ 0.7740, -0.0026, -0.0785]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "class ELU:\n",
    "    def __init__(self, alpha: float) -> None:\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self, inputs: th.Tensor) -> th.Tensor:\n",
    "        \n",
    "        # <реализовать логику ELU>\n",
    "        # inputs: (batch_size, n_features)\n",
    "        # returns: (batch_size, n_features)\n",
    "        return th.where(inputs>=0,inputs,self.alpha * (th.exp(inputs) - 1))\n",
    "    \n",
    "inputs = th.randn(4,3)\n",
    "    \n",
    "elu = ELU(0.1)\n",
    "elu.forward(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02fb0d",
   "metadata": {
    "id": "0peh8r-20Pof"
   },
   "source": [
    "### Создание функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab7e56a",
   "metadata": {
    "id": "uaR7rILd1eWR"
   },
   "source": [
    "<p class=\"task\" id=\"5\"></p>\n",
    "\n",
    "5 Используя операции над матрицами и векторами из библиотеки `torch`, реализуйте функцию потерь CrossEntropyLoss:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://i.ibb.co/93gy1dN/Screenshot-9.png\" width=\"200\">\n",
    "\n",
    "$$ CrossEntropyLoss = \\frac{1}{n}\\sum_{i=1}^{n}{L_i}$$\n",
    "где $y_i$ - вектор правильных ответов для примера $i$, $\\hat{y_i}$ - вектор предсказаний модели для примера $i$; $k$ - количество классов, $n$ - количество примеров в батче. Для определения слоя не используйте готовые функции или слои из `torch`, реализующие данную функцию потерь.\n",
    "\n",
    "Создайте полносвязный слой с 2 нейронами и прогнать через него батч `inputs`. Полученный результат пропустите через функцию активации Softmax. Посчитайте значение функции потерь, трактуя вектор `y` как вектор правильных ответов. \n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f683f102",
   "metadata": {
    "id": "hQl8pJsT3HcF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7596)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, y_pred: th.Tensor, y_true: th.Tensor) -> th.Tensor:\n",
    "        # <реализовать логику функции потерь>\n",
    "        # y_pred: (batch_size, n_classes)\n",
    "        # y_true: (batch_size, n_classes)\n",
    "        # returns: scalar\n",
    "        return (-1/ y_true.shape[0]) * (y_true * th.log(y_pred)).sum()\n",
    "\n",
    "\n",
    "inputs = th.randn(7,5)\n",
    "\n",
    "lin = Linear(2,inputs.shape[1], True)\n",
    "res = lin.forward(inputs)\n",
    "\n",
    "act = Softmax()\n",
    "res = act.forward(res)\n",
    "\n",
    "ce = CrossEntropyLoss()\n",
    "loss = ce.forward(res[:,0].reshape(-1,1),res[:,1].reshape(-1,1))\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9c82a1",
   "metadata": {
    "id": "fA6dbanf44_4"
   },
   "source": [
    "<p class=\"task\" id=\"6\"></p>\n",
    "\n",
    "6 Модифицируйте MSE, добавив L2-регуляризацию.\n",
    "\n",
    "$$MSE_R = MSE + \\lambda\\sum_{i=1}^{m}w_i^2$$\n",
    "\n",
    "где $\\lambda$ - коэффициент регуляризации; $w_i$ - веса модели.\n",
    "\n",
    "- [ ] Проверено на семинаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1f858a9",
   "metadata": {
    "id": "ADsZxD-h4_Os"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6723)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as th\n",
    "class MSERegularized:\n",
    "    def __init__(self, lambda_: float) -> None:\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def data_loss(\n",
    "        self, \n",
    "        y_pred: th.Tensor, \n",
    "        y_true: th.Tensor,\n",
    "    ) -> th.Tensor:\n",
    "        # <подсчет первого слагаемого из формулы>\n",
    "        # y_pred: (batch_size, )\n",
    "        # y_true: (batch_size, )\n",
    "        return (y_true - y_pred).pow(2).mean()\n",
    "\n",
    "    def reg_loss(self, weights: th.Tensor) -> th.Tensor:\n",
    "        # <подсчет второго слагаемого из формулы>\n",
    "        # weights: (batch_size, 1)\n",
    "        return self.lambda_ * weights.pow(2).sum()\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        y_pred: th.Tensor, \n",
    "        y_true: th.Tensor, \n",
    "        weights: th.Tensor,\n",
    "    ) -> th.Tensor:\n",
    "        # y_pred: (batch_size,)\n",
    "        # y_true: (batch_size,)\n",
    "        # weights: (batch_size, 1)\n",
    "        return self.data_loss(y_pred, y_true) + self.lambda_ * self.reg_loss(weights)\n",
    "    \n",
    "\n",
    "y_pred = th.tensor([1.0, 3.0, 5.0])\n",
    "y_true = th.tensor([2.0, 3.0, 4.0])\n",
    "\n",
    "MSERegularized(0.2).forward(y_pred,y_true, th.tensor([0.1,0.2,0.3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

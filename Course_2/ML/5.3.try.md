Here are the solutions to the self-study tasks based on the provided document.

### Задание 1: Визуализация распределения признаков

Визуализация распределения признаков и совместного распределения признаков и целевой переменной с помощью тепловых карт и других инструментов визуализации.

**Python**

```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Загрузка датасета Customer Support (предполагается, что файл CS_data.csv доступен)
# CS_data = pd.read_csv('CS_data.csv')

# Пример визуализации распределения категориального признака 'channel_name' с помощью гистограммы
sns.histplot(data=CS_data, x="channel_name")
plt.title('Распределение channel_name')
plt.show()

# Пример визуализации распределения категориального признака 'category' с помощью упорядоченной столбчатой диаграммы
counts = CS_data.category.value_counts()
sns.barplot(x=counts.index, y=counts.values)
plt.xticks(rotation=45, ha='right')
plt.title('Распределение category')
plt.tight_layout()
plt.show()

# Пример визуализации совместного распределения 'channel_name' и 'CSAT Score' с помощью столбчатой диаграммы средних значений
sns.catplot(data=CS_data, x="channel_name", y="CSAT Score", kind="bar")
plt.title('Средний CSAT Score по channel_name')
plt.show()

# Пример визуализации совместного распределения 'category' и 'CSAT Score'
sns.catplot(data=CS_data, x="category", y="CSAT Score", kind="bar", aspect=2)
plt.xticks(rotation=45, ha='right')
plt.title('Средний CSAT Score по category')
plt.tight_layout()
plt.show()

# Пример построения тепловой карты для категориальных признаков (требуется предварительное кодирование, например, get_dummies)
# Пример для subset категориальных признаков с небольшим количеством уникальных значений
categorical_cols = ['channel_name', 'Manager', 'Tenure Bucket', 'Agent Shift']
# Создание dummy-переменных
CS_dummies_subset = pd.get_dummies(CS_data[categorical_cols])

# Добавление целевой переменной
CS_dummies_subset['CSAT Score'] = CS_data['CSAT Score']

# Расчет матрицы корреляций
correlation_matrix = CS_dummies_subset.corr()

# Построение тепловой карты корреляций
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm')
plt.title('Тепловая карта корреляций между категориальными признаками (после get_dummies) и CSAT Score')
plt.show()

# Визуализация распределения признака с большим количеством уникальных значений (например, Customer_City) с помощью таблицы value_counts()
# print(CS_data['Customer_City'].value_counts().head(20))
```

### Задание 2: Построение модели дерева решений и анализ важности признаков

Построение модели дерева решений на получившемся датасете Customer support после предобработки и анализ важности признаков для выводов об адекватности предположений.

**Python**

```
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeRegressor
import pandas as pd
# Импорт необходимых библиотек
import pandas as pd

# Предполагается, что CS_dummies - это датасет после выполнения всех шагов предобработки из методички
# CS_dummies = pd.read_csv('CS_data_processed.csv') # Загрузка предобработанного датасета

# Разделение на признаки (X) и целевую переменную (y)
X = CS_dummies.drop('CSAT Score', axis=1)
y = CS_dummies['CSAT Score']

# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Создание и обучение модели дерева решений
model = DecisionTreeRegressor(random_state=42)
model.fit(X_train, y_train)

# Анализ важности признаков
feature_importances = model.feature_importances_

# Создание DataFrame для удобного просмотра важности признаков
feature_importance_df = pd.DataFrame({
    'feature': X_train.columns,
    'importance': feature_importances
})

# Сортировка признаков по важности
feature_importance_df = feature_importance_df.sort_values('importance', ascending=False)

# Вывод наиболее важных признаков
print("Важность признаков:")
print(feature_importance_df.head(10))

# Визуализация важности признаков (опционально)
# plt.figure(figsize=(10, 6))
# sns.barplot(x='importance', y='feature', data=feature_importance_df.head(10))
# plt.title('Топ 10 важных признаков (Дерево решений)')
# plt.show()
```

### Задание 3: Разделение датасета и преобразование выборок

Разбивка датасета на тестовую и обучающую выборки и преобразование обеих подвыборок с использованием параметров, полученных на обучающей выборке.

**Python**

```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.composeimport ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Загрузка исходного датасета Customer Support (предполагается, что файл CS_data.csv доступен)
# CS_data = pd.read_csv('CS_data.csv')

# Выделение категориальных и числовых признаков (учитывая, какие признаки останутся после удаления лишних)
# Список категориальных признаков, которые будут кодироваться One-Hot Encoder
categorical_features = ['channel_name', 'category', 'Product_category', 'Manager', 'Tenure Bucket', 'Agent Shift']

# Список числовых признаков (включая новые агрегированные и бинаризованные, если они были созданы)
# Здесь предполагается, что 'connected_handling_time' был заполнен и, возможно, преобразован в числовой тип,
# а 'Customer Remarks New', 'Is_order', 'Agent_count', 'Sups_no_agents' уже числовые.
# Важно: перед этим шагом необходимо провести все этапы укрупнения категорий, добавления агрегированной информации и заполнения пропусков.
# Для примера, возьмем признаки из предобработанного датасета CS_dropped из методички
# (предполагается, что этот датасет был создан на предыдущих шагах)
# CS_dropped = CS_data.drop([...список удаленных столбцов...], axis=1)
# CS_dropped['Customer Remarks New'] = (CS_dropped["Customer Remarks"].str.len() > 3).astype(int)
# CS_dropped["Is_order"] = (CS_dropped["Order_id"].isna()).astype(int)
# CS_dropped['Agent_count'] = CS_dropped.groupby(["Agent_name"])["Agent_name"].transform('count')
# CS_dropped['Sups_no_agents'] = CS_dropped.groupby(["Supervisor"])["Agent_name"].transform('nunique')
# CS_dropped['Product_category'] = CS_dropped['Product_category'].fillna('unknown')
# CS_dropped['connected_handling_time'] = pd.to_numeric(CS_dropped['connected_handling_time'], errors='coerce').fillna(0) # Пример заполнения и преобразования

# Определим финальный список признаков после предобработки, исключая целевую переменную
features = ['channel_name', 'category', 'Product_category', 'connected_handling_time',
            'Manager', 'Tenure Bucket', 'Agent Shift', 'Customer Remarks New',
            'Is_order', 'Agent_count', 'Sups_no_agents']
target = 'CSAT Score'

# Разделение на признаки (X) и целевую переменную (y)
X = CS_dropped[features]
y = CS_dropped[target]


# Разделение на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Определение колонок для One-Hot Encoding
one_hot_features = ['channel_name', 'category', 'Product_category', 'Manager', 'Tenure Bucket', 'Agent Shift']

# Создание ColumnTransformer для применения One-Hot Encoding только к категориальным колонкам
preprocessor = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(handle_unknown='ignore'), one_hot_features)
    ],
    remainder='passthrough' # Остальные колонки (числовые) оставить как есть
)

# Создание пайплайна, включающего препроцессор
pipeline = Pipeline(steps=[('preprocessor', preprocessor)])

# Применение преобразования к обучающей выборке
X_train_transformed = pipeline.fit_transform(X_train)

# Применение преобразования к тестовой выборке (используя параметры, полученные на обучающей выборке)
X_test_transformed = pipeline.transform(X_test)

# Получение названий колонок после One-Hot Encoding (для удобства, если нужно создать DataFrame)
# ohe_feature_names = pipeline.named_steps['preprocessor'].named_transformers_['onehot'].get_feature_names_out(one_hot_features)
# all_feature_names = np.concatenate([ohe_feature_names, X_train.columns.drop(one_hot_features)])

# X_train_transformed_df = pd.DataFrame(X_train_transformed, columns=all_feature_names, index=X_train.index)
# X_test_transformed_df = pd.DataFrame(X_test_transformed, columns=all_feature_names, index=X_test.index)

# print("Размерность обучающей выборки после преобразования:", X_train_transformed.shape)
# print("Размерность тестовой выборки после преобразования:", X_test_transformed.shape)
```

### Задание 4: Полный анализ на датасете Титаник

Проведение полного анализа на датасете Титаник, включая все необходимые визуализации и выводы.

**Python**

```
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

# Загрузка датасета Титаник (предполагается, что файл titanic.csv доступен)
# T_data = pd.read_csv('titanic.csv')

# Вывод основной информации о датасете
# print("Основная информация о датасете Титаник:")
# T_data.info()

# Вывод описательной статистики для категориальных признаков
# print("\nОписательная статистика для категориальных признаков:")
# print(T_data.describe(exclude=[np.number]))

# Визуализация распределения категориальных признаков
# sns.countplot(data=T_data, x='Sex')
# plt.title('Распределение по полу')
# plt.show()

# sns.countplot(data=T_data, x='Pclass')
# plt.title('Распределение по классу обслуживания')
# plt.show()

# sns.countplot(data=T_data, x='Embarked')
# plt.title('Распределение по порту посадки')
# plt.show()

# Визуализация связи категориальных признаков с целевой переменной 'Survived'
# sns.catplot(data=T_data, x='Sex', y='Survived', kind='bar')
# plt.title('Выживаемость по полу')
# plt.show()

# sns.catplot(data=T_data, x='Pclass', y='Survived', kind='bar')
# plt.title('Выживаемость по классу обслуживания')
# plt.show()

# sns.catplot(data=T_data, x='Embarked', y='Survived', kind='bar')
# plt.title('Выживаемость по порту посадки')
# plt.show()

# Преобразование бинарного признака 'Sex'
# LE_sex = LabelEncoder()
# T_data['Sex_encoded'] = LE_sex.fit_transform(T_data['Sex'])
# print("\nДатасет после кодирования Sex:")
# print(T_data[['Sex', 'Sex_encoded']].head())

# Преобразование порядкового признака 'Pclass' (если он в текстовом формате)
# Пример, если Pclass загружен как 'first', 'second', 'third'
# T_data['Pclass_encoded'] = T_data['Pclass'].replace({
#     'first': 1, 'second': 2, 'third': 3
# })
# print("\nДатасет после кодирования Pclass:")
# print(T_data[['Pclass', 'Pclass_encoded']].head())

# Заполнение пропусков в 'Embarked' (например, модой)
# most_frequent_embarked = T_data['Embarked'].mode()[0]
# T_data['Embarked_filled'] = T_data['Embarked'].fillna(most_frequent_embarked)
# print("\nКоличество пропусков в Embarked до заполнения:", T_data['Embarked'].isnull().sum())
# print("Количество пропусков в Embarked после заполнения:", T_data['Embarked_filled'].isnull().sum())

# Преобразование номинального признака 'Embarked' с помощью OneHotEncoder
# OH_embarked = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
# embarked_dummies = pd.DataFrame(OH_embarked.fit_transform(T_data[['Embarked_filled']]),
#                                columns=OH_embarked.get_feature_names_out(['Embarked_filled']),
#                                index=T_data.index)
# T_data_processed = pd.concat([T_data, embarked_dummies], axis=1).drop(['Embarked', 'Embarked_filled'], axis=1)
# print("\nДатасет после One-Hot Encoding Embarked:")
# print(T_data_processed.head())

# Пример удаления лишних столбцов
# T_data_final = T_data_processed.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)
# print("\nФинальный датасет Титаник после предобработки:")
# print(T_data_final.head())
# print("\nИнформация о финальном датасете Титаник:")
# T_data_final.info()
```

### Задание 5: Проверка целесообразности преобразований

Проверка целесообразности каждого необязательного преобразования данных путем проверки, увеличивает ли данное преобразование точность модели (линейная регрессия, дерево решений или случайный лес).

**Python**

```
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensembleimport RandomForestRegressor
from sklearn.metrics import mean_squared_error
fromsklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Загрузка исходного датасета Customer Support (предполагается, что файл CS_data.csv доступен)
# CS_data_original = pd.read_csv('CS_data.csv')

# Копия для проведения экспериментов
# CS_data_experiment = CS_data_original.copy()

# Определение целевой переменной и базовых признаков
# target = 'CSAT Score'
# base_features = ['connected_handling_time', 'Customer Remarks New', 'Is_order', 'Agent_count', 'Sups_no_agents']
# Категориальные признаки для экспериментов
# experimental_categorical_features = ['channel_name', 'category', 'Sub-category', 'Customer_City', 'Product_category', 'Manager', 'Tenure Bucket', 'Agent Shift']

# Пример функции для оценки модели
# def evaluate_model(X_train, X_test, y_train, y_test, model):
#     model.fit(X_train, y_train)
#     predictions = model.predict(X_test)
#     rmse = np.sqrt(mean_squared_error(y_test, predictions))
#     return rmse

# Эксперимент 1: Базовая модель без дополнительных категориальных признаков (только числовые и бинаризованные)
# X_base = CS_data_experiment[base_features]
# y = CS_data_experiment[target]
# X_train_base, X_test_base, y_train, y_test = train_test_split(X_base, y, test_size=0.2, random_state=42)
# lr_base = LinearRegression()
# rmse_lr_base = evaluate_model(X_train_base, X_test_base, y_train, y_test, lr_base)
# print(f"RMSE базовой линейной регрессии: {rmse_lr_base}")

# Эксперимент 2: Добавление One-Hot Encoding для 'channel_name'
# features_exp2 = base_features + ['channel_name']
# X_exp2 = CS_data_experiment[features_exp2].copy()
# X_exp2 = pd.get_dummies(X_exp2, columns=['channel_name'], drop_first=True) # drop_first=True для избежания мультиколлинеарности
# X_train_exp2, X_test_exp2, y_train, y_test = train_test_split(X_exp2, y, test_size=0.2, random_state=42)
# lr_exp2 = LinearRegression()
# rmse_lr_exp2 = evaluate_model(X_train_exp2, X_test_exp2, y_train, y_test, lr_exp2)
# print(f"RMSE линейной регрессии с channel_name (OHE): {rmse_lr_exp2}")

# Проведение аналогичных экспериментов для других категориальных признаков и комбинаций,
# а также для других моделей (DecisionTreeRegressor, RandomForestRegressor)
# Сравнение метрик RMSE для разных вариантов преобразований и на разных моделях.
# Выводы о том, какие преобразования улучшают качество модели.
```

### Задание 6: Создание воспроизводимого кода обработки датасета

Создание воспроизводимого кода для полной обработки данного датасета, включающего все необходимые шаги предобработки категориальных признаков.

**Python**

```
strong
```
